#!/usr/bin/env python3
#
# MIT License
#
# s3-pit-restore, a point in time restore tool for Amazon S3
#
# Copyright (c) [2016] [Madisoft S.p.A.]
#
# Author: Matteo Moretti <matteo.moretti@madisoft.it>
# Author: Angelo Compagnucci <angelo.compagnucci@gmail.com>
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import os, sys, time, signal, argparse, boto3, botocore, \
        unittest, concurrent.futures, shutil, uuid, time
from datetime import datetime, timezone
from dateutil.parser import parse
from s3transfer.manager import TransferConfig

args = None
executor = None
transfer = None
futures = {}

class TestS3PitRestore(unittest.TestCase):

    def generate_tree(self, path, contents):
        for i, content in enumerate(contents):
            folder_path = os.path.join(path, "folder%d" % i)
            os.makedirs(folder_path, exist_ok=True)
            file_path = os.path.join(folder_path, "file%d" % i)

            with open(file_path, 'w') as outfile:
                outfile.write(content)
            print(file_path, content)

    def check_tree(self, path, contents):
        for i, content in enumerate(contents):
            folder_path = os.path.join(path, "folder%d" % i)
            os.makedirs(folder_path, exist_ok=True)
            file_path = os.path.join(folder_path, "file%d" % i)

            in_content=""
            with open(file_path, 'r') as infile:
                in_content = infile.read()
                print(file_path, content, "==", in_content)
                if in_content != content:
                    return False
        return True

    def upload_directory(self, resource, path, bucketname):
        with concurrent.futures.ThreadPoolExecutor(args.max_workers) as e:                
            for root,dirs,files in os.walk(path):
                for f in files:
                    base_path = os.path.basename(os.path.normpath(path))
                    local_path = os.path.join(root, f)
                    relative_path = os.path.relpath(local_path, path)
                    s3_path = os.path.join(base_path, relative_path)
                    e.submit(resource.meta.client.upload_file, os.path.join(root,f), bucketname, s3_path)

    def test_restore(self):
        contents_before = [ str(uuid.uuid4()) for n in range(2048) ]
        contents_after =  [ str(uuid.uuid4()) for n in range(2048) ]
        path = os.path.join(os.path.abspath(args.dest), "test-s3-pit-restore")
        s3 = boto3.resource('s3')
        bucket_versioning = s3.BucketVersioning(args.bucket)
        bucket_versioning.load()

        print("Checking bucket versioning ... ", end='', flush=True)
        self.assertNotEqual(bucket_versioning.status, None)
        print("enabled!")

        print("Before ...")
        shutil.rmtree(path)
        time_before = datetime.now(timezone.utc)
        time.sleep(1)
        self.generate_tree(path, contents_before)
        self.upload_directory(s3, path, args.bucket)
        shutil.rmtree(path)

        print("Upload and owerwriting ...")
        time.sleep(1)
        time_after = datetime.now(timezone.utc)
        self.generate_tree(path, contents_after)
        self.upload_directory(s3, path, args.bucket)
        shutil.rmtree(path)

        args.from_timestamp = str(time_before)
        args.timestamp = str(time_after)
        args.prefix = os.path.basename(os.path.normpath(path))
        do_restore()
        print("Restoring and checking ...")
        self.assertTrue(self.check_tree(path, contents_before))

def signal_handler(signal, frame):
    executor.shutdown(wait=False)
    for future in list(futures.keys()):
        if not future.running():
            future.cancel()
            futures.pop(future, None)
    print("Gracefully exiting ...")

def print_obj(obj, optional_message=""):
    if args.verbose:
        print('"%s" %s %s %s %s %s' % (obj["LastModified"], obj["VersionId"], obj["Size"], obj["StorageClass"], obj["Key"], optional_message))
    else:
        print(obj["Key"])

def handled_by_glacier(obj):
    if obj["StorageClass"] == "GLACIER" and not args.enable_glacier:
        print_obj(obj)
        return True
    elif obj["StorageClass"] == "GLACIER" and args.enable_glacier:
        s3 = boto3.resource('s3')
        s3_obj = s3.Object(args.bucket, obj["Key"])
        if s3_obj.restore is None:
            print_obj(obj, optional_message='requesting')
            if not args.dry_run:
                try:
                    s3_obj.restore_object(VersionId=obj["VersionId"], RestoreRequest={'Days': 3},)
                except botocore.exceptions.ClientError as ex:
                    # sometimes s3_obj.restore returns None also if restore is in progress
                    pass
            return True
        # Print out objects whose restoration is on-going
        elif 'ongoing-request="true"' in s3_obj.restore:
            print_obj(obj, optional_message='in-progress')
            return True
        # Print out objects whose restoration is complete
        elif 'ongoing-request="false"' in s3_obj.restore:
            return False
    else:
        return False

def handled_by_standard(obj):
    if obj["Key"].endswith("/"):
        if not os.path.exists(obj["Key"]):
            os.makedirs(obj["Key"])
        return True
    key_path = os.path.dirname(obj["Key"])
    if key_path and not os.path.exists(key_path):
            os.makedirs(key_path)
    try:
        if not args.dry_run:
            future = executor.submit(download_file, obj)
            global futures
            futures[future] = obj
        else:
            print_obj(obj)
    except RuntimeError:
        return False
    return True

def download_file(obj):
    transfer.download_file(args.bucket, obj["Key"], obj["Key"], extra_args={"VersionId": obj["VersionId"]})
    unixtime = time.mktime(obj["LastModified"].timetuple())
    os.utime(obj["Key"],(unixtime, unixtime))

def do_restore():
    pit_start_date = (parse(args.from_timestamp) if args.from_timestamp else datetime.fromtimestamp(0, timezone.utc))
    pit_end_date = (parse(args.timestamp) if args.timestamp else datetime.now(timezone.utc))
    client = boto3.client('s3', endpoint_url=args.endpoint_url)
    global transfer
    transfer = boto3.s3.transfer.S3Transfer(client)
    dest = args.dest
    last_obj = {}
    last_obj["Key"] = ""

    if args.debug: boto3.set_stream_logger('botocore')

    global executor
    executor = concurrent.futures.ThreadPoolExecutor(args.max_workers)

    if not os.path.exists(dest):
        os.makedirs(dest)

    os.chdir(dest)

    # AWS gives us versions chunks of maximum 1000 element, cycling here to obtain more
    objects = client.list_object_versions(Bucket=args.bucket, Prefix=args.prefix) # cannot pass VersionIdMarker on first call
    while (True):
        if not "Versions" in objects:
            print("No versions matching criteria, exiting ...", file=sys.stderr)
            sys.exit(1)
        versions = objects["Versions"]
        deletemarkers = objects.get("DeleteMarkers", [])
        dmarker = {"Key":""}
        for obj in versions:
            if last_obj["Key"] == obj["Key"]:
                # We've had a newer version or a delete of this key
                continue

            version_date = obj["LastModified"]

            if version_date > pit_end_date or version_date < pit_start_date:
                # Object was not updated during pit window
                continue

            while deletemarkers and (dmarker["Key"] < obj["Key"] or dmarker["LastModified"] > pit_end_date):
                dmarker = deletemarkers.pop(0)

            if dmarker["Key"] == obj["Key"] and dmarker["LastModified"] > obj["LastModified"]:
                # The mostresent operation on this key was a delete
                last_obj = dmarker
                continue

            # This version needs to be restored..
            last_obj = obj

            if handled_by_glacier(obj):
                continue

            if not handled_by_standard(obj):
                return

        for future in concurrent.futures.as_completed(futures):
            if future in futures:
                try:
                    future.result()
                    print_obj(futures[future])
                except Exception as ex:
                    print('"%s" %s %s %s %s "ERROR: %s"' % (obj["LastModified"], obj["VersionId"], obj["Size"], obj["StorageClass"], obj["Key"], ex), file=sys.stderr)
                del(futures[future])

        if objects["IsTruncated"]:
            # More objects to be got
            objects = client.list_object_versions(Bucket=args.bucket, Prefix=args.prefix, \
                KeyMarker=objects["NextKeyMarker"], VersionIdMarker=objects["NextVersionIdMarker"])
        else:
            break


if __name__=='__main__':
    signal.signal(signal.SIGINT, signal_handler)

    parser = argparse.ArgumentParser()
    parser.add_argument('-b', '--bucket', help='s3 bucket to restore from', required=True)
    parser.add_argument('-p', '--prefix', help='s3 path to restore from', default="")
    parser.add_argument('-t', '--timestamp', help='final point in time to restore at')
    parser.add_argument('-f', '--from-timestamp', help='starting point in time to restore from')
    parser.add_argument('-d', '--dest', help='path where recovering to', required=True)
    parser.add_argument('-u', '--endpoint-url', help='Endpoint URL to connect to', default=None)
    parser.add_argument('-k', '--skip-ssl-validation', help='If set, SSL validation is skipped', action='store_true')
    parser.add_argument('-e', '--enable-glacier', help='enable recovering from glacier', action='store_true')
    parser.add_argument('-v', '--verbose', help='print verbose informations from s3 objects', action='store_true')
    parser.add_argument('--dry-run', help='execute query without transferring files', action='store_true')
    parser.add_argument('--debug', help='enable debug output', action='store_true')
    parser.add_argument('--test', help='s3 pit restore testing', action='store_true')
    parser.add_argument('--max-workers', help='max number of concurrent download requests', default=10, type=int)
    args = parser.parse_args()

    if not args.test:
        do_restore()
    else:
        runner = unittest.TextTestRunner()
        itersuite = unittest.TestLoader().loadTestsFromTestCase(TestS3PitRestore)
        runner.run(itersuite)

    sys.exit(0)
